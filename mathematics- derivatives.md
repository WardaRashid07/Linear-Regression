Lets start with a simple dataset
assume values of x and y to be 

                         x = [0.5 , 2.3 , 2.9 ]                         

                         y = [1.4 , 1.9 , 3.2 ]
                         
suppose the x-values to be the weight of individuals and y-values to be height                        


# 1. Hypothesis Function (The Line Equation)

The best fit line in simple linear regression is modeled as:

**≈∑ = mx + c**  
or  
**≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx**

Where:  
- **≈∑**: Predicted output  
- **x**: Input feature  
- **Œ≤‚ÇÄ**: Intercept (equivalent to c)  
- **Œ≤‚ÇÅ**: Slope (equivalent to m)

---
<img src ="4_prediction.png" width = "300"> 


In multiple linear regression, the hypothesis function becomes:

**≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ‚ãØ + Œ≤‚Çôx‚Çô**

# 2. Cost Function (Loss Function)

## In Simple Terms

In linear regression, the **loss function** (also called the cost function) is a formula used to measure how bad the model‚Äôs predictions are ‚Äî essentially, how far off the predicted values are from the actual values in your dataset.

It tells us: **‚ÄúHow wrong is the model?‚Äù**

The goal of linear regression is to find the **line (model)** that minimizes this error ‚Äî meaning it fits the data as closely as possible.


## Most Common Loss/Cost Function in Linear Regression

**Mean Squared Error (MSE):**


To measure how good the line is, we use **Mean Squared Error (MSE)**:

**MSE = (1/n) * Œ£·µ¢=1‚Åø (y·µ¢ ‚àí ≈∑·µ¢)¬≤**

Where:

- **y·µ¢** = Actual value  
- **≈∑·µ¢** = Predicted value from the line ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx  
- **n** = Number of data points


     $$J(Œ≤‚ÇÄ, Œ≤‚ÇÅ) =  &nbsp;&nbsp;&nbsp;(1/n) * Œ£·µ¢=1‚Åø (y·µ¢ - ≈∑·µ¢)¬≤ &nbsp;&nbsp;&nbsp;= (1/n) * Œ£·µ¢=1‚Åø (y·µ¢ - (Œ≤‚ÇÄ + Œ≤‚ÇÅx·µ¢))¬≤$$

**Notation: `J(Œ≤‚ÇÄ, Œ≤‚ÇÅ)`**

- **`J`**: A conventional symbol commonly used to represent a cost or loss function.
- **`Œ≤‚ÇÄ`, `Œ≤‚ÇÅ`**: Model parameters (intercept and slope). These are inputs to the cost function, and the computed cost depends on their values.
  

**$$(1/n) * Œ£·µ¢=1‚Åø (y·µ¢ - ≈∑·µ¢)¬≤$$**  this represents that the actual value of y minus the predicted value of y
,square it , do it for all values of acutal and predicted y, and then add them up. Now divide it by total 
number of y values which is labeled as n .


if we replace **≈∑** with **Œ≤‚ÇÄ + Œ≤‚ÇÅx** it becomes **$$(1/n) * Œ£·µ¢=1‚Åø (y·µ¢ - (Œ≤‚ÇÄ + Œ≤‚ÇÅx·µ¢))¬≤$$**

This is  our cost function and our goal is to minimize this error.

---

## How It Works

1. For each point, calculate the difference between actual and predicted value:  
   **(y·µ¢ ‚àí ≈∑·µ¢)**

2. Square that answer to:
   - Remove negative signs  
   - Penalize large errors more heavily

3. then add them up and divide the answer with total 
number of y values which is labeled as n  to get MSE ‚Äî this is your **cost**.

---
<img src="5%20-%20R%20square%20-%20residual%20definition.png" width="300">

The red point represents the acutal data point. The dotted line is connected to the horizontal line at 
some point, this point is called aas the predicted value.

---

## Why Square the Error?

- It emphasizes larger mistakes.
- Prevents positive and negative errors from canceling each other out.

---

## Goal in Linear Regression

Minimize the cost function (usually MSE) by adjusting:
- **Œ≤‚ÇÄ** = Intercept  
- **Œ≤‚ÇÅ** = Slope

### 3. Optimization Method

### Gradient Descent (Iterative Method)

1. **Initialize Parameters**  
   Start by assigning random initial values to `Œ≤‚ÇÄ` = 0 and `Œ≤‚ÇÅ` = 1.

2. **Compute Gradients (Partial Derivatives)**  
   For each iteration, calculate:

   - ‚àÇJ/‚àÇŒ≤‚ÇÄ = ‚àÇŒ£(y·µ¢ ‚àí ≈∑·µ¢)¬≤ / ‚àÇŒ≤‚ÇÄ
   - ‚àÇJ/‚àÇŒ≤‚ÇÅ = ‚àÇŒ£(y·µ¢ ‚àí ≈∑·µ¢)¬≤ / ‚àÇŒ≤‚ÇÄ
     

3. **Update Weights**  
   Adjust the parameters using the gradients:

   - Step size = intercept * 0.01
     
     0.01 is a constant value called learning rate
     
      new intercept = old intercept - step size
     
   - Step size = slope * 0.01
     
     0.01 is a constant value called learning rate
     
      new slope = old slope - step size 

4. **Repeat Until Convergence**  
   Continue updating until the cost function `J(Œ≤‚ÇÄ, Œ≤‚ÇÅ)` reaches its minimum (i.e., the changes become negligible).

#### üìò Notes

- `Œ±` is the **learning rate** ‚Äî it controls the step size during each update.
- `≈∑·µ¢` is the predicted value: `≈∑·µ¢ = Œ≤‚ÇÄ + Œ≤‚ÇÅ * x·µ¢`


